<%= javascript_include_tag 'our_methods.js' %>
<h1> Our Methods </h1>

<br>
<h3>Content</h3>
<ol>
    <li><a href="#syn_gen">Synthetic data generation</a></li>
    <li><a href="#eval_met">Evaluation Methods</a></li>
    <li><a href="#data_handle">Data Handling</a></li>
    <li><a href="#references">References</a></li>
</ol>
<hr>

<h3 style="color:#0275D8" id="syn_gen">Synthetic data generation</h3>

<!--
<p>The <i>(Two?)</i> Generative Adversarial Network model developed to generate Synthetic Data is based on recent work (Jarret et al. 2019, Lin et al. 2019, DP papers that helped design the DP in our GAN).
</p>

<p>Briefly <i>...text about how the GAN works/the steps including the differential privacy – have Lulu’s scheme here and reference to it as you go along?) (…the functioning of the GAN can be divided into 4 main sections…)
<br>… The GAN requires continuous time-series for all individuals. Missing time-series elements are imputed but masked, with the synthetic data produced mimicking the time-series variability. <br> …To ensure privacy and anonymity, differential privacy was embedded in the GAN, based on previous work (references)…</i>
</p>

<p>The model trained is one that is capable of preserving the static and temporal dynamics of data. To generate realistic time-series data, unsupervised learning of a traditional GAN is combined with supervised learning of time-steps within the data. In addition, since temporal dynamics of systems are often driven by lower-dimensional factors of variation, an embedding network is used for reversible mapping between original features to a latent dimension. Joint training is then done on both the embedding and generator network such that the supervised loss is minimised.
</p>
-->

<p>Synthetic data is generated using a Generative Adversarial Network (GAN). Our algorithm is based on the paper “Generating High-fidelity, Synthetic Time Series Datasets with DoppelGANger” (Lin et al. 2019). The model trained is one that is capable of preserving the static and time series features of the data. Our algorithm can learn the correlations between temporal features and their attributes (static values). Long term correlations within time-series can also be preserved.</p>

<h5 style="color:#472F91">The Data</h5>
<!--<p>We are able to accommodate static, time-series and mixed data. In the PRISM dataset we present here, each participant had x static and x temporal features including binary, categorical and numerical types. ... add more stuff about how flexible it is...</p>-->
<p>Our model is flexible and able to deal with different types of data. We are able to accommodate static, time-series and mixed data. In the PRISM dataset we present here, each participant had static features such as household ID and temporal features such as weight. The dataset also contains numerical data that is continuous such as weight and categorical data, such as whether a patient has a cough. The categorical data are one-hot-encoded before being fed into the model. </p>
<p>When dealing with time-series data, a model usually requires the same time-series length for each sample of data. In our dataset however, each participant visited the hospital a different number of times. Our model is able to cope with these different number of visits and is capable of capturing the distribution of the number of visits participants make.</p>
<p>Recurrent Neural Networks that are used in the model expects a regularly-spaced time-series data for all individuals. However, medical data of participants tend to be irregular, i.e. participants do not tend to visit the hospital every day. To account for the missing time-steps, an additional input time, delta time, column is fed into the algorithm so that the algorithm is above to generate the time in which an event occurs.</p>
<!--...stuff about how flexible it is…-->
<p>Data obtained is first divided into attributes and temporal features. The temporal features are then classified into numerical and categorical data, in which categorical data is then one-hot-encoded. Missing values in the data are filled in with interpolated values. The data is then batched and each sample of the batched data represents a participant.</p>

<h5 style="color:#472F91">The Algorithm</h5>
<!--
<p>The embedding network consists of the auto-encoder and auto-decoder. The generative adversarial network (GAN) consists of a generator to generate synthetic data and discriminator to differentiate between the real and synthetic data.</p>
-->
<!-- This only works on regular html files <img src="GAN-scheme.jpg" height="500">-->
<!--
<figure>
<%= image_tag("GAN-scheme-2.png", :alt => "GAN", :height => "500") %>
<figcaption><i>Schematic of the synthetic data generator</i></figcaption>
</figure>

<ol>
  <li><b>Embedder Network training with differential privacy.</b> The encoder takes the original data and reduces its dimension and the decoder does the opposite. This allows the adversarial network to learn the data in a lower latent dimension. The reduction in dimensionality is motivated by the fact that complex systems often rely on fewer lower-dimensional factors of variation. The network trains by comparing the imperfect reconstruction to the input and adjusts its parameters to make them more similar. The purpose of the embedder is to distill the data, helping the GAN to train on a representation of the original data. Because this network ‘sees’ the original data during training, we add differential privacy to prevent it from retaining identifying information.
    <br>(We can also reduce the dimensionality of the data by removing columns and leaving those that are strongly correlated to it...)</li>
  <li><b>Training with supervised loss.</b>Supervised loss is introduced to explicitly encourage the model to capture timestep-wise distributions in the data. The model learns the transition dynamics of the real sequence, in addition to simply just learning whether the data is real or synthetic. Supervised loss is calculated between the data generated by the model at each time-step while using the original data as supervision. This training is done in the latent dimension.
  </li>
  <li><b>Joint training.</b> The generator produces output in the latent encoded dimensions, instead of in the original features dimension. It is implemented through a recurrent neural network. The generator approximates a distribution of the encoded data and is fed random noise in order to sample from this distribution. A sample of real data is also encoded. The real and synthetic encoded data are given to the discriminator which is scored on how accurately it is able to distinguish them. The discriminator adjusts its parameters to maximise the chances of correctly classifying between the real and synthetic encoded data. The generator on the other hand learns to generate more life-like encoded data to fool the discriminator, trying to minimise the likelihood of the discriminator providing the correct classification. Supervised training is again done here as an incentive for the generator to capture the timestep-wise distributions in the data. The generator and discriminator are trained in alternating periods.
  <br>This part of the GAN works only within the (latent) space of representations of the real data. The generator is trained on feedback of the discriminator and does not see the original data so does not need to be trained with differential privacy. Saving on our privacy budget here preserves more utility in the final output.
  <br>The GAN works only within the (latent) space of representations of the real data. The generator is trained on feedback of the discriminator and does not see the original data so does not need to be trained with differential privacy. Saving on our privacy budget here preserves more utility in the final output.
  </li>
  <li><b>Generate the synthetic data.</b> More random noise is used to sample from the final distribution learned by the generator. The generator generates data in the latent space, in which the decoder then reforms it into the dimensions of the original dataset.
  </li>
</ol>
-->

<p>The figure below shows the architecture of the GAN we are using. There are three parts in data generation, i.e. generating attributes, min/max and features. The discriminator is used to discriminate the entire sample. An additional discriminator is used to evaluate attributes and min/max.</p>
<figure>
<%= image_tag("doppelganger.png", :alt => "GAN", :width => "70%") %>
<figcaption><i>Architecture of the synthetic data generator</i></figcaption>
</figure>

<p>The GAN is made of two competing networks - the generator and discriminator - both improving together as we train. 
   They are adversarial in the game theory sense, and are playing a zero-sum game. 
   The generator tries to generate realistic looking data while the discriminator is shown real data and synthetic data from the generator and its job is to classify them as either real or fake. 
   In training, the generator updates its weights to maximize the probability that fake data is being classified as real, which is the opposite of what the discriminator does. 
   Once the discriminator starts to only classify the data right about half the time, we know that the generator is generating realistic looking data such that the discriminator is not able to distinguish the real and fake samples. </p>
<p>An additional discriminator, called the Auxiliary discriminator, is added specifically for classifying attributes. 
   This helps split up the problem and gives the generator better feedback in order to improve fidelity.</p>
<p>The algorithm is capable of capturing the correlation between attributes (static values) and temporal features. 
   To enable the model to capture these correlations, data generation is separated into 2 phases: first by generating the attributes, 
   and then the features are generated based on the attributes to capture these dependencies.</p>
<p>In order to alleviate mode collapse, data is first normalised. 
  Then for each feature the data has, combinations derived from the minimum and maximum for each sample are used as added attributes. 
  These added attributes are also used to scale features back to a realistic range.</p>
<p>To capture temporal correlations with long time-series length, MLPs and RNNs are used together in generation. 
  The MLP network reads the output from the RNN and generates a batch of samples in the desired dimensionality, 
  and the RNN iterates over each sample to produce consecutive records that make up the whole time series. </p>
<p>The model is also capable of dealing with different time-series length, i.e. participants visiting the hospital a different number of times. 
  A generation flag was added to indicate whether a time series has ended. And if the time series has ended, the generator stops generating data. 
  The generation flags are also fed to the discriminator as part of the features so that the discriminator can learn sample time-series length characteristics.</p>

<h5 style="color:#472F91">Privacy</h5>
<p>Synthetic data has huge advantages over anonymised data when it comes to privacy. We use differential privacy to guarantee the generated data cannot be used to expose the data of any individual or even to conclude that the individual was in the original dataset. Instead of removing details from real data, synthetic data can accurately preserve the detail/coarseness and distribution to enable query design and refinement.</p>
<p>Differential privacy is achieved by:</p>
<ul>
  <li>Clipping gradients during training</li>
  <li>Using averages of gradients</li>
  <li>Adding random noise to the gradients before applying changes</li>
</ul>
<p>These measures reduce the impact a single individual’s data can have on the weights by being in the training set. As a result, their information is neither retained by the network nor generated in the synthetic data. Using differential privacy during training rather than post processing makes our synthetic data resilient against membership inference attacks even if trained components of the network are exposed.</p>
<p>We use TensorFlow Privacy (4) to implement these changes.</p>

<hr>

<h3 style="color:#0275D8" id="eval_met">Evaluation methods</h3>
<p> The Synthetic Data Generator used by Syndasera was tested on the publicly available Program for Resistance, Immunology, Surveillance and Modelling of Malaria (PRISM) dataset. This dataset includes time-series and static features amongst categorical and numerical variables, with 54 columns and over 40,000 rows, providing a robust variety of elements.
</p>

<p>Three main things are evaluated in synthesised synthetic data to ensure optimum utility whilst not compromising privacy:
    1) <u>Distribution</u> - The synthetic data should capture the diversity, distribution and dependencies between variables of the original data;
    1) <u>Fidelity</u> – The synthetic data should be as similar to the original data as possible without imitating samples, without compromising privacy;
    2) <u>Usefulness</u> – As the user will always need to confirm observations made on the synthetic data with the original data, synthetic data should behave in a similar way to the original data when applied to algorithms for data analysis;
    3) <u>Privacy</u> – No real individual should be identifiable from the synthetic data.
    <br>
    To execute the evaluation mehtods, 20% of the original data, not used in the generative model, are set aside (original evaluation sample). An equal size of the synthetic data is used for comparison (synthetic evaluation sample).
    <br>
<!--    <%= image_tag("DWPPre.png", :alt => "GAN", :height => "400") %> -->
    <br>
    To see an example of the application of these methods, <a href="/prism_evaluation">click here</a>.
</p>


<ol>
    <li><u>Distribution.</u> To assess distribution, the following methods are applied:
        <ol>
            <br>
            <li><b>Numerical variables distribution comparison.</b>
                <br>
                For numerical columns, histograms for the original dataset and the generated dataset are overlaid for each column, giving a visual perception of the similarity in diversity and distribution.
                <br>To quantify this, a Kolmogrov-Smirnov Test is performed with 100 randomly selected samples to avoid biasing the result towards rejecting the null hypothesis.
                The Kolmogrov-Smirnov Test is a non parametric hypothesis test where the null hypothesis is that two samples (one from the original and one from the synthetic) originate from the same distribution.
                <br><b>The closer the Kolmogrov-Smirnov Test p-value is to 1, the higher the fidelity</b>
                <div id="histogram_mockup"></div>
            </li>
            <br>
            <li><b>Categorical variables distribution comparison.</b>
                <br>
                For categorical columns, the empirical distribution of each variable for all categorical columns is calculated for the synthetic dataset and the original dataset.
                <br>A dot plot compares the empirical distribution of each variable for the original data set on the x axis and for the synthetic dataset on the y axis, giving a visual perception of the similarity in diversity and distribution.
                The closer the dot to the y=x line, the higher the fidelity.
                <br>To quantifiy this, the MSE (mean standard error) between the empirical distribution for the original and synthetic dataset is calculated for each variable and averaged.
                <br><b>The lower the averaged MSE, the higher the fidelity.</b>
                <div id="barchart_mockup"></div>
                <div id="empirical_plot"></div>
            </li>
            <br>
            <li><b>Pearson's Correlation Coefficient comparison.</b>
                <br>
                To confirm dependencies between variables within columns are maintained, Pearson’s R correlation coefficient is calculated between categorical (all?) columns
                within the original and synthetic data.
                <br>To compare the datasets, we use two measurements. Firstly, the MSE (mean standard error) is calculated between the correlation coefficient between original and the generated data, for each column with the other columns. The averge is presented here.
                <br><b>The lower the MSE, the higher the fidelity.</b>
                <br>Secondly, the SRA (Synthetic Ranking Accuracy) for each column is calculated. Here, the R correlation coefficients for each column with the other columns is ranked for both the synthetic and the original data and compared. The average is presented here.
                <br><b>The closer SRA is to 1 the higher the fidelity.</b>
            </li>
            <br>
        </ol>
    </li>
    <li><u>Fidelity.</u> To assess fidelity, the following methods are applied:
        <ol>
            <br>
            <li><b>tSNE and PCA plots comparison.</b>
                <br>
                tSNE and PCA plots reduce a multi-dimensional dataset (i.e with multiple columns) into a 2-dimensional visualisation.
                A machine-learning algorithm groups similar variables and although we cannot interpret the coordinates attributed,
                we can compare if the groupings made are similar in the original and synthetic data by overlaying the plots.
                <br><b>The higher the overlay, the higher the fidelity.</b>
                <div id="tSNE_mockup"></div>
            </li>
            <br>
            <li><b>Discriminative model performance</b>
                <br>
                For a quantitative measure of similarity, a post-hoc time-series classification model is trained (by optimizing a multi-layer GRU)
                to distinguish between sequences from the original and generated datasets.
                First, each original sequence is labeled '1', and each generated sequence is labeled '0'.
                Then, an off-the-shelf (RNN) classifier is trained to distinguish between the two classes as a standard supervised task.
                An accuracy score on the held-out test set gives a quantitative assessment of fidelity.
                <br><b>An accuracy score similar to the expected accuracy score for an untrained discriminative model indicates the discriminator cannot tell the difference between both datasets.</b>
            </li>
            <br>
            <li><b>Autocorrelation comparison.</b>
                <br>
                To confirm the variance in numerical values/counts over time are maintained, autocorrelation graphs are plotted for both datasets and overlayed.
                <br><b>The higher the overlay, the higher the fidelity.</b>
                <div id="Autocorrelation_mockup"></div>
            </li>
            <br>
        </ol>
    </li>
    <li><u>Predictiveness.</u> To assess predictiveness, the following methods are applied:
        <ol>
            <br>
            <li><b>Predictive model performance comparison</b>
                The original and generated evaluation samples are further divided into a test sample (90% of evaluation sample) and train sample (10% of evaluation sample).
                <!-- Numerical data is transformed to categorical data and all data is one-hot-encoded, including NA values as a category (exception: if NA% &lt3% these rows are excluded).  -->
                A series of commonly used regression based predictive models (Linear Regression, Linear Support Vector Regression, Random Forests Regression, Logistic Regression, Decision tree Regression and K-Nearest Neighbors Regression) are applied to the test samples and trained on the train samples for both the original and the synthetic data (Test-on-real, Train-on-real & Test-on-synthetic, Train-on-synthetic). The model attempts to predict a variable within a column based on the remaining columns.
                The MSE (mean standard error) is calculated between the predicted value resulting from the predictive model and the test sample and averaged (MSE_1), resulting in MSE_1's for each column, for each predictive model, for the original and synthetic datasets (visualised in the first scatter plot below).
                <br>To compare the datasets, the MSE between the MSE_1's of the original and the synthetic dataset is calculated (MSE_2), resulting in MSE_2's for each predictive model.
                <br>The average of MSE_1's for all columns is calculated between the datasets (MSE_3), resulting in MSE_3 for each predictive model, for the original and the synthetic dataset (visualised in the second scatter plot below).
                <br> To compare datasets, the MSE between the MSE_3's for the original and the synthetic data is calculated (MSE_4).
                <br><b>A low MSE_2/MSE_4 indicates the synthetic and original dataset behaved similarly in the predictive models.</b>
                <br>Because users to do not have access to the original data, synthetic data should behave similarly to the original data when submitted to any algorithms.
                To asses this, the MSE_1's are ranked across each predictive model for the original and synthetic data and compared using the SRA (Synthetic Ranking Accuracy) measure (SRA_1), resulting in SRA_1 for each column.
                <br>Similarly, the MSE_3's for each predictive model can be ranked, giving an SRA_2 measure.
                <br><b>SRA values close to 1 indicate the synthetic and original dataset behave similarly in predictive models.</b>
                <ul>
                    <li>Scatter plot of MSE_1
                        <div id="predictive_mockup"></div>
                    </li>
                    <li>Scatter plot of MSE_3
                        <div id="MSE3_plot"></div>
                    </li>
                </ul>
            </li>
            <br>
            <li><b>Next-step predictive model performance comparison (time-series only). </b>
                <br>
                This method has a similar principal to the previous method with the difference that the predictive model attempts to predict all variables for next time-step entry
                based on the values for previous time-stamps.
                <br>Here the predictive model provided by Tensorflow Keras API returns a test metric and a test loss for each column.
                <br>To compare between datasets, two parameters are determined.
                <br>Firstly, the MSE between the test loss/test metric is calculated between the real and the synthetic data for each column.
                <br>Secondly, the SRA is calculated, ranking the test metric/test loss of each column, comparing the rank for the synthetic and for the original data.
                <br><b>A low MSE and an SRA close to 1 indicates the predictive model behaves similarly in attempting to predict the next time-step in both datasets.</b>
                <div id="tpredictive_mockup"></div>
            </li>
            <br>
        </ol>
    </li>
    <li><u>Privacy.</u> Including differential privacy within the GAN ensures total anonymity. However, to further test this, membership inference attacks are performed.
    </li>
</ol>
<hr>

<h3 style="color:#0275D8" id="data_handle">Data handling</h3>

<p>
Data shared between clients and Syndasera will be held on a Data Access Environment. Generation of Synthetic Data and it’s evaluation will be performed in this secure server using Databricks so that no data is transferred onto a local machine.
</p>

<hr>

<h3 style="color:#0275D8" id="references">References</h3>

<ol>
<li>Jarrett, D., Yoon, J., Van de Schaar, M. (2019), ‘Time-series Generative Adversarial Networks’, Advances in Neural Information Processing Systems 32, Vancouver Canada.</li>
<li>Lin, Z.,  Jain, A., Wang, C., Fanti, G., Sekar, V. (2019), ‘Generating High-fidelity, Synthetic Time Series Datasets with DoppelGANger’, <i>arXiv e-prints</i>, arXiv:1909.13403</li>
<li>Jordan, J., Yoon, J., Van de Schaar, M. (2018), ‘Measuring the quality of Synthetic data for use in competitions’ <i>arXiv e-prints</i>, arXiv:1806.11345</li>
<li>(2019), 'TensorFlow Privacy 0.3.0' <i>https://github.com/tensorflow/privacy<i>
</ol>

<!--<a href="/make_request">text</a>-->
<!--<a href="/prism_dashboard">text</a>-->
<!--<a href="/prism_evaluation">text</a>-->

<script>



///////////////////////



</script>
