<h1> About Syndasera </h1>
<h3>Synthetic data generation</h3>

<p>The <i>(Two?)</i> Generative Adversarial Network model developed to generate Synthetic Data is based on recent work (Jarret et al. 2019, Lin et al. 2019, DP papers that helped design the DP in our GAN).
</p>

<p>Briefly <i>...text about how the GAN works/the steps including the differential privacy – have Lulu’s scheme here and reference to it as you go along?) (…the functioning of the GAN can be divided into 4 main sections…)
<br>… The GAN requires continuous time-series for all individuals. Missing time-series elements are imputed but masked, with the synthetic data produced mimicking the time-series variability. <br> …To ensure privacy and anonymity, differential privacy was embedded in the GAN, based on previous work (references)…</i>
</p>

<hr>

<h3>Evaluation methods</h3>
<p> The Synthetic Data Generator used by Syndasera was tested on the publicly available Program for Resistance, Immunology, Surveillance and Modelling of Malaria (PRISM) dataset. This dataset includes time-series and static features amongst categorical and numerical variables, with 54 columns and over 1million rows, providing a robust variety of elements.
</p>

<p>Three main things are evaluated in synthesised synthetic data to ensure optimum utility whilst not compromising privacy:
  1) <u>Fidelity</u> – the synthetic data should capture the diversity, distribution and dependencies between variables of the original data;
  2) <u>Predictiveness</u> – As the user will always need to confirm observations made on the synthetic data with the original data, synthetic data should behave in a similar way to the original data when applied to algorithms for data analysis;
  3) <u>Privacy</u> – No real individual should be identifiable from the synthetic data.
  <br>
  To execute the evaluation mehtods, 20% of the original data, not used in the generative model, are set aside (original evaluation sample). An equal size of the synthetic data is used for comparison (synthetic evaluation sample).
  <br>
  To see an example of the application of these methods, <a href="/prism_dashboard">click here</a>.
</p>

<ol>
  <li><u>Fidelity.</u> To assess fidelity, the following methods are applied:</li>
      <ol>
        <li><b>tSNE and PCA plots.</b> tSNE and PCA plots (Jarrett et al. 2019). Need to write a description for this </li>
        <li><b>Empirical distributions comparison.</b> The empirical distribution (dictionary) for each variable within the dataset is calculated for the synthetic and the real data set. To compare the datasets, the MSE (mean standard error(dictionary)) is calculated between each empirical distribution for each variable between both datasets and averaged. The lower the averaged MSE, the higher the fidelity.
            <br>The empirical distribution comparison can also be specifically calculated for a column of interest. For example, in a cancer registry database, it may be of interest to subset the registry by cancer type and then do an empirical distribution comparison across the remaining columns.</li>
        <li><b>Inter-variable dependencies.</b> To confirm dependencies between variables within columns are maintained, Pearson’s correlation coefficient <i>(link dictionary)</i> is calculated between categorical columns within the original and synthetic data. <i>Not sure how we will assess similarity yet</i></li>
        <li><b>Autocorrelation comparison.</b> To confirm the variance in numerical values/counts over time are maintained, autocorrelation graphs are plotted for both datasets and compared.</li>
      </ol>
  <li><u>Predictiveness.</u> To assess predictiveness, the following methods are applied:</li>
      <ol>
        <li><b>Predictive model performance.</b> The original and synthetic evaluation samples are further divided into a test sample (90% of evaluation sample) and train sample (10% of evaluation sample). Numerical data is transformed to categorical data and all data is one-hot-encoded, including NA values as a category (exception: if NA% &lt3% these rows are excluded). A series of commonly used predictive models (Random Forests, Logistic regression…) are applied to the test samples and trained on the train samples for both the real and the synthetic data (Test-on-real, Train-on-real and Test-on-synthetic, Train-on-synthetic) in accordance to previous studies (reference final papers where the models were taken from). The predictive model results in an accuracy score for the ability of the model to predict each variable from all others.
        <br>To compare the datasets, the MSE (mean standard error<i>(dictionary)</i>) is calculated between the accuracy scores for each variable between both datasets and averaged. A low averaged MSE indicates the synthetic and original dataset behaved similarly in the predictive models.
        <br>Parallel to the averaged MSE, the Synthetic Accuracy Ranking is calculated, where the accuracy scores are used to rank each predictive model. If both datasets behave similarly the ranking will be equal. <i>(still not 100% sure about this description).</i></li>
        <li><b>Next-step predictive model performance (time-series only). </b></li>
      </ol>
  <li><u>Privacy.</u> Including differential privacy within the GAN ensures total anonymity. However, to further test this, the following methods are applied:</li>
      <ol>
        <li><b>Hack tests.</b></li>
      </ol>
</ol>

<hr>

<h3>Data handling</h3>

<p>
  Data shared between users and Syndasera will be held on a Data Access Environment specifically the London Server <i>(I made this bit up)</i>. Generation of Synthetic Data and it’s evaluation will be performed in this secure server, so that no data is transferred onto a local machine.
  <i>More stuff to say here?</i>
</p>

<hr>

<h3>References</h3>

<p>
Jarrett, D., Yoon, J., Van de Schaar, M. (2019), ‘Time-series Generative Adversarial Networks’, Advances in Neural Information Processing Systems 32, Vancouver Canada.
<br>
Lin, Z.,  Jain, A., Wang, C., Fanti, G., Sekar, V. (2019), ‘Generating High-fidelity, Synthetic Time Series Datasets with DoppelGANger’, arXiv e-prints, arXiv:1909.13403
<br>
Jordan, J., Yoon, J., Van de Schaar, M. (2018), ‘Measuring the quality of Synthetic data for use in competitions’ arXiv e-prints, arXiv:1806.11345
</p>



<!--<a href="/make_request">text</a>-->
<!--<a href="/prism_dashboard">text</a>-->
